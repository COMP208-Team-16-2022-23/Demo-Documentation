# Data Processing

Data processing is a crucial step in analyzing experimental data using LCDA. Data collected from experiments may contain
various issues, such as noise, skewness, and outliers, which may affect the accuracy and reliability of the results.
Therefore, it is important to process the data before analysis.

This chapter will guide you through the steps of data processing. Whether you are a novice or an experienced user, this
chapter provides valuable information and guidance to help you obtain accurate and reliable results using LCDA.

## Data Processing Projects

### Create a new project

Before beginning data processing in LCDA, you will need to create a new data processing project. This can be done by
clicking on the `NEW PROCESSING` button located in the upper right corner of the `Data Processing` interface.

![](/images/data-processing/data-processing-index.jpg)

In the pop-up window, you will need to select a dataset (Want to upload your own dataset? Please refer to the
[My Data](./my-data.md#upload-files) user manual to learn how to upload a dataset). Then click on the `START PROCESSING`
button in the lower right corner to start data processing. The system will automatically create a data processing
project for you.

Here we use the `iris.csv` dataset to demonstrate the process of data processing.

![](/images/data-processing/data-processing-new-processing.jpg)

### Continue editing a project

If you have already created a data processing project, the data processing interface will list all of your data
processing projects. You can click on the `EDIT` button on the right to continue editing an existing data processing
project.

![](/images/data-processing/data-processing-index-edit.jpg)

### Delete a project

If you no longer need a data processing project, you can click on the `DELETE` button on the right to delete the
project.

> **Note**: Deleting data processing items is a permanent operation and cannot be recovered after deletion. This also
> deletes the dataset generated by the last run in the project.
> Please make sure you have saved the data you need before deleting the project.

![](/images/data-processing/data-processing-index-delete.jpg)

## Data Processing Steps

1. After clicking the `EDIT` button or creating a new data processing project, the data processing interface is
   displayed. From here, you can work with the data.

   ![](/images/data-processing/data-processing-project-page.jpeg)

   In the data processing interface, you will find a list of data processing algorithms on the left-hand side. You can
   select the algorithm you want to use by clicking on it, and then set the parameters of the algorithm in the pop-up
   panel. The specific parameters for each algorithm may vary, so please refer to
   the [Data Processing Algorithms](#data-processing-algorithms) for more information on how to set them.

   ![](/images/data-processing/data-processing-algorithm-list.jpg)

2. Here, we will demonstrate the process of using the `Outlier handling` algorithm as an example. Once you have selected
   the algorithm and set the parameters, you can click on the `Start processing` button located in the lower right
   corner of the panel to initiate the algorithm.

   ![](/images/data-processing/data-processing-parameter-panel.jpg)

3. Once the algorithm has completed processing the data, the output will be displayed on the right-hand side of the
   interface.

   ![](/images/data-processing/data-processing-data-preview.jpg)

4. At the same time, the processed dataset will be saved and added to your list of datasets. To view or download your
   new dataset, you can navigate to the `My Data` page.

   ![](/images/data-processing/data-processing-my-data-result.jpg)

## Data Processing Algorithms

### Outlier handling

#### Description

Outlier handling algorithms are used to identify and handle outliers in a dataset. In statistics, an outlier is a data
point that significantly differs from other observations, and may be caused by experimental error or other factors.
Outliers can cause serious problems in statistical analysis and can affect the accuracy and reliability of results.
Therefore, outlier handling algorithms are essential for identifying and dealing with these problematic data points.
These algorithms can help improve the quality of the dataset and ultimately lead to more accurate and reliable analysis
results.

> [Outlier - Wikipedia](https://en.wikipedia.org/wiki/Outlier)

#### Parameters

- `Detection method`: Outlier detection method. currently supported
  [3-sigma](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule), [IQR](https://en.wikipedia.org/wiki/Interquartile_range),
  and [MAD](https://en.wikipedia.org/wiki/Median_absolute_deviation) three methods.
- `Processing method`: Outlier processing method. Currently supports `set to null`, `set to mean`, and `set to median`
  three methods.

### Missing value handling

#### Description

Missing data can result in biased or inaccurate results if not handled properly. Missing value handling algorithms can
be used to impute the missing data with reasonable estimates, based on statistical methods such as mean imputation,
median imputation. These methods can help to reduce the bias and improve the accuracy of statistical analysis.

> [Missing data - Wikipedia](https://en.wikipedia.org/wiki/Missing_data)

#### Parameters

- `identification method`: Missing value detection method. Currently supports `empty`, `space`, `None` and `Non-numeric`
  four methods.
- `filling method`: missing value processing method. Currently supports `mean`, `median`, and `mode` three methods.

### Tail shrinkage and truncation processing

#### Description

Tail shrinkage and truncation processing algorithms are used to shrink the tails of a distribution. This can be useful
when the tails of a distribution are too long, which can cause problems in statistical analysis. These algorithms can
help to reduce the bias and improve the accuracy of statistical analysis.


> [Truncation (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Truncation_(statistics))
>
> [Shrinkage (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Shrinkage_(statistics))

#### Parameters

- `method_selection`: Tail shrinkage and truncation processing method. Currently supports `tail_shrinkage`
  and `tail_truncation` two methods.
- `upper_limit`: Upper limit of the tail shrinkage and truncation processing. Data types are numeric.
- `lower_limit`: Lower limit of the tail shrinkage and truncation processing. Data types are numeric.
- `processing_method`: Tail shrinkage and truncation processing method. Currently supports `delete_value`
  and `delete_row` two methods.

### Data transformation

#### Description

In statistics, data transformation is the application of a deterministic mathematical function to each point in a data
set—that is, each data point z is replaced with the transformed value y = f(z), where f is a function. Transforms are
usually applied so that the data appear to more closely meet the assumptions of a statistical inference procedure that
is to be applied, or to improve the interpretability or appearance of graphs.

> [Data transformation (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Data_transformation_(statistics))

#### Parameters

- `transform_method`: Data conversion method. Currently, [FFT](https://en.wikipedia.org/wiki/Fast_Fourier_transform) and
  IFFT (Inverse Fast Fourier Transform) are supported.

### Dimension reduction

#### Description

Data dimensionality reduction is the process of reducing the number of dimensions or variables in a high-dimensional
dataset, while retaining the essential information. The aim is to simplify the dataset and eliminate the irrelevant or
redundant variables, making it easier to process and analyze. The process involves transforming the data into a
lower-dimensional space, while still preserving the key characteristics of the data. This technique is particularly
useful for dealing with large datasets where high dimensionality can lead to issues with computational efficiency and
overfitting.

> [Dimensionality reduction - Wikipedia](https://en.wikipedia.org/wiki/Dimensionality_reduction)

#### Parameters

- `method`: Dimensionality reduction method. Currently
  supports [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)
  and [LDA](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) two methods.
- `n_components`: Number of components to keep. Data types are numeric.

### Sample balancing

#### Description

Sample balance refers to the process of adjusting the number of samples in each category of a dataset so that they are
more evenly distributed. This is important in machine learning and statistical analysis because imbalanced datasets can
lead to biased results, especially when dealing with rare events or minority classes. By balancing the samples, we can
improve the performance and accuracy of the algorithms that use the dataset.

> [Oversampling and undersampling in data analysis - Wikipedia](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis)

#### Parameters

- `balancing_method`: Sample balancing method. Currently
  supports `undersample` ([RandomUnderSampler](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html)),
  `oversample` ([RandomOverSampler](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html))
  and `combined` （[SMOTEENN](https://imbalanced-learn.org/stable/references/generated/imblearn.combine.SMOTEENN.html))
  three methods.

### Normalization

#### Description

The purpose of standardization is to make the dataset easier to compare and analyze. Standardization also helps to
remove the units of measurement from the data, making it possible to compare variables that have different units.

> [Normalization (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Normalization_(statistics))

#### Parameters

- `Method`: Normalization method. Currently supports [Min_Max](https://en.wikipedia.org/wiki/Feature_scaling) and
  [Z_Score](https://en.wikipedia.org/wiki/Standard_score) two methods.
