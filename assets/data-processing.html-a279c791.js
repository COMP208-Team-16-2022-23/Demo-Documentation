import{_ as s,a as d,b as l,c,d as h,e as p,f as m,g as u,h as g,i as f,j as _,k as b,l as y,m as w,n as v,o as k}from"./Example_Normalization-7b12f06b.js";import{_ as x,M as r,p as D,q as T,R as e,t as a,N as t,U as P,a1 as n}from"./framework-efe98465.js";const S={},j=n('<h1 id="data-processing" tabindex="-1"><a class="header-anchor" href="#data-processing" aria-hidden="true">#</a> Data Processing</h1><p>Data processing is a crucial step in analyzing experimental data using LCDA. Data collected from experiments may contain various issues, such as noise, skewness, and outliers, which may affect the accuracy and reliability of the results. Therefore, it is important to process the data before analysis.</p><p>This chapter will guide you through the steps of data processing. Whether you are a novice or an experienced user, this chapter provides valuable information and guidance to help you obtain accurate and reliable results using LCDA.</p><h2 id="data-processing-projects" tabindex="-1"><a class="header-anchor" href="#data-processing-projects" aria-hidden="true">#</a> Data Processing Projects</h2><h3 id="create-a-new-project" tabindex="-1"><a class="header-anchor" href="#create-a-new-project" aria-hidden="true">#</a> Create a new project</h3><p>Before beginning data processing in LCDA, you will need to create a new data processing project. This can be done by clicking on the <code>NEW PROCESSING</code> button located in the upper right corner of the <code>Data Processing</code> interface.</p><p><img src="'+d+'" alt=""></p>',7),z=e("code",null,"START PROCESSING",-1),I=n('<p>Here we use the <code>iris.csv</code> dataset to demonstrate the process of data processing.</p><p><img src="'+l+'" alt=""></p><h3 id="continue-editing-a-project" tabindex="-1"><a class="header-anchor" href="#continue-editing-a-project" aria-hidden="true">#</a> Continue editing a project</h3><p>If you have already created a data processing project, the data processing interface will list all of your data processing projects. You can click on the <code>EDIT</code> button on the right to continue editing an existing data processing project.</p><p><img src="'+c+'" alt=""></p><h3 id="delete-a-project" tabindex="-1"><a class="header-anchor" href="#delete-a-project" aria-hidden="true">#</a> Delete a project</h3><p>If you no longer need a data processing project, you can click on the <code>DELETE</code> button on the right to delete the project.</p><div class="custom-container warning"><p class="custom-container-title">WARNING</p><p><strong>Note</strong>: Deleting data processing items is a permanent operation and cannot be recovered after deletion. This also deletes the dataset generated by the last run in the project. Please make sure you have saved the data you need before deleting the project.</p></div><p><img src="'+h+'" alt=""></p><h2 id="data-processing-steps" tabindex="-1"><a class="header-anchor" href="#data-processing-steps" aria-hidden="true">#</a> Data Processing Steps</h2><ol><li><p>After clicking the <code>EDIT</code> button or creating a new data processing project, the data processing interface is displayed. From here, you can work with the data.</p><p><img src="'+p+'" alt=""></p><p>In the data processing interface, you will find a list of data processing algorithms on the left-hand side. You can select the algorithm you want to use by clicking on it, and then set the parameters of the algorithm in the pop-up panel. The specific parameters for each algorithm may vary, so please refer to the <a href="#data-processing-algorithms">Data Processing Algorithms</a> for more information on how to set them.</p><p><img src="'+m+'" alt=""></p></li><li><p>Here, we will demonstrate the process of using the <code>Outlier handling</code> algorithm as an example. Once you have selected the algorithm and set the parameters, you can click on the <code>Start processing</code> button located in the lower right corner of the panel to initiate the algorithm.</p><p><img src="'+u+'" alt=""></p></li><li><p>Once the algorithm has completed processing the data, the output will be displayed on the right-hand side of the interface.</p><p><img src="'+g+'" alt=""></p></li><li><p>At the same time, the processed dataset will be saved and added to your list of datasets. To view or download your new dataset, you can navigate to the <code>My Data</code> page.</p><p><img src="'+f+'" alt=""></p></li></ol><h2 id="data-processing-algorithms" tabindex="-1"><a class="header-anchor" href="#data-processing-algorithms" aria-hidden="true">#</a> Data Processing Algorithms</h2><h3 id="outlier-handling" tabindex="-1"><a class="header-anchor" href="#outlier-handling" aria-hidden="true">#</a> Outlier Handling</h3><h4 id="description" tabindex="-1"><a class="header-anchor" href="#description" aria-hidden="true">#</a> Description</h4><p>Outlier handling algorithms are used to identify and handle outliers in a dataset. In statistics, an outlier is a data point that significantly differs from other observations, and may be caused by experimental error or other factors. Outliers can cause serious problems in statistical analysis and can affect the accuracy and reliability of results. Therefore, outlier handling algorithms are essential for identifying and dealing with these problematic data points. These algorithms can help improve the quality of the dataset and ultimately lead to more accurate and reliable analysis results.</p><p>Outlier handling algorithms can use various methods for identification and management, including statistical-based, machine learning-based, and deep learning-based approaches. Common statistical-based methods include the 3Ïƒ rule, box plot method, etc. Machine learning-based methods include Local Outlier Factor (LOF), Isolation Forest, etc., while deep learning-based methods include Autoencoder, etc.</p><p>When dealing with outliers, it is necessary to choose the appropriate algorithm based on the specific scenario. Sometimes, certain outliers may contain useful information, and therefore require careful consideration and management. In general, through appropriate outlier handling algorithms, the quality of the dataset can be improved, and the accuracy of the results can be enhanced.</p>',17),N={href:"https://en.wikipedia.org/wiki/Outlier",target:"_blank",rel:"noopener noreferrer"},O=e("h4",{id:"parameters",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#parameters","aria-hidden":"true"},"#"),a(" Parameters")],-1),C=e("code",null,"Detection method",-1),E={href:"https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule",target:"_blank",rel:"noopener noreferrer"},M={href:"https://en.wikipedia.org/wiki/Interquartile_range",target:"_blank",rel:"noopener noreferrer"},W={href:"https://en.wikipedia.org/wiki/Median_absolute_deviation",target:"_blank",rel:"noopener noreferrer"},F=e("li",null,[e("code",null,"Processing method"),a(": Outlier processing method. Currently supports "),e("code",null,"set to null"),a(", "),e("code",null,"set to mean"),a(", and "),e("code",null,"set to median"),a(" three methods.")],-1),q=n('<h4 id="previews" tabindex="-1"><a class="header-anchor" href="#previews" aria-hidden="true">#</a> Previews</h4><p><img src="'+_+'" alt=""></p><h3 id="missing-value-handling" tabindex="-1"><a class="header-anchor" href="#missing-value-handling" aria-hidden="true">#</a> Missing Value Handling</h3><h4 id="description-1" tabindex="-1"><a class="header-anchor" href="#description-1" aria-hidden="true">#</a> Description</h4><p>Missing value processing refers to the process of handling missing values in a dataset. In data analysis and machine learning tasks, missing values are a common problem. The existence of missing values may be due to errors in the data collection process, human omissions, or other factors. If missing values are not handled, they will affect the accuracy and reliability of data analysis and machine learning models.</p><p>Methods for missing value processing include deletion, imputation, and prediction. Deletion refers to directly deleting data rows or columns that contain missing values. This method may reduce the amount of data but may make the dataset incomplete and lose useful information. Imputation refers to using existing data to infer missing values. Imputation methods include mean imputation, median imputation, regression imputation, and interpolation. Prediction refers to using machine learning models to predict missing values. This method requires training machine learning models with existing data and then using the models to predict missing values.</p><p>When choosing a missing value processing method, factors to consider include the nature of the data, the cause of data missing, and the bias that processing methods may introduce. Through appropriate missing value processing methods, the accuracy and reliability of data analysis and machine learning models can be improved.</p>',7),A={href:"https://en.wikipedia.org/wiki/Missing_data",target:"_blank",rel:"noopener noreferrer"},L=n('<h4 id="parameters-1" tabindex="-1"><a class="header-anchor" href="#parameters-1" aria-hidden="true">#</a> Parameters</h4><ul><li><code>identification method</code>: Missing value detection method. Currently supports <code>empty</code>, <code>space</code>, <code>None</code> and <code>Non-numeric</code> four methods.</li><li><code>filling method</code>: missing value processing method. Currently supports <code>mean</code>, <code>median</code>, and <code>mode</code> three methods.</li></ul><h4 id="previews-1" tabindex="-1"><a class="header-anchor" href="#previews-1" aria-hidden="true">#</a> Previews</h4><p><img src="'+b+'" alt=""></p><h3 id="tail-shrinkage-and-truncation-processing" tabindex="-1"><a class="header-anchor" href="#tail-shrinkage-and-truncation-processing" aria-hidden="true">#</a> Tail Shrinkage and Truncation Processing</h3><h4 id="description-2" tabindex="-1"><a class="header-anchor" href="#description-2" aria-hidden="true">#</a> Description</h4><p>When there is sufficient sample data, it is common to perform shrinkage or truncation on continuous variables to eliminate the influence of extreme values on research. The purpose of shrinkage or truncation is to make the extreme values in the dataset closer to the central value, thereby making the dataset more consistent with the normal distribution. The data values that fall outside of a specific percentile range for the variable are processed after being sorted from smallest to largest. The standard for processing is below the lower limit and beyond the upper limit. Shrinkage replaces these values with their specific percentile values, while truncation directly deletes the values.</p>',7),R={href:"https://en.wikipedia.org/wiki/Truncation_(statistics)",target:"_blank",rel:"noopener noreferrer"},B={href:"https://en.wikipedia.org/wiki/Shrinkage_(statistics)",target:"_blank",rel:"noopener noreferrer"},H=n('<h4 id="parameters-2" tabindex="-1"><a class="header-anchor" href="#parameters-2" aria-hidden="true">#</a> Parameters</h4><ul><li><code>method_selection</code>: Tail shrinkage and truncation processing method. Currently supports <code>tail_shrinkage</code> and <code>tail_truncation</code> two methods.</li><li><code>upper_limit</code>: Upper limit of the tail shrinkage and truncation processing. Data types are numeric.</li><li><code>lower_limit</code>: Lower limit of the tail shrinkage and truncation processing. Data types are numeric.</li><li><code>processing_method</code>: Tail shrinkage and truncation processing method. Currently supports <code>delete_value</code> and <code>delete_row</code> two methods.</li></ul><h4 id="previews-2" tabindex="-1"><a class="header-anchor" href="#previews-2" aria-hidden="true">#</a> Previews</h4><p><img src="'+y+'" alt=""></p><h3 id="data-transformation" tabindex="-1"><a class="header-anchor" href="#data-transformation" aria-hidden="true">#</a> Data Transformation</h3><h4 id="description-3" tabindex="-1"><a class="header-anchor" href="#description-3" aria-hidden="true">#</a> Description</h4><p>Data transformation refers to a process of manipulating or processing the data in the original dataset to extract or modify the information or features contained therein. In the fields of data analysis and machine learning, data transformation is an important step in data preprocessing, which can be used for tasks such as data dimensionality reduction, feature extraction, anomaly detection, and data smoothing.</p>',7),U={href:"https://en.wikipedia.org/wiki/Data_transformation_(statistics)",target:"_blank",rel:"noopener noreferrer"},V=e("h4",{id:"parameters-3",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#parameters-3","aria-hidden":"true"},"#"),a(" Parameters")],-1),G=e("code",null,"transform_method",-1),Y={href:"https://en.wikipedia.org/wiki/Fast_Fourier_transform",target:"_blank",rel:"noopener noreferrer"},Q=n('<h4 id="previews-3" tabindex="-1"><a class="header-anchor" href="#previews-3" aria-hidden="true">#</a> Previews</h4><p><img src="'+w+'" alt=""></p><h3 id="dimension-reduction" tabindex="-1"><a class="header-anchor" href="#dimension-reduction" aria-hidden="true">#</a> Dimension Reduction</h3><h4 id="description-4" tabindex="-1"><a class="header-anchor" href="#description-4" aria-hidden="true">#</a> Description</h4><p>Dimensionality reduction refers to the process of mapping data from a high-dimensional space to a low-dimensional space while preserving the essential features of the original dataset. In the field of data analysis and machine learning, high-dimensional datasets often lead to a significant increase in computational complexity and difficulty in visualizing and interpreting the data. Therefore, dimensionality reduction can effectively reduce the dimensionality of the dataset, improve data processing efficiency and interpretability, and also reduce the risk of overfitting and improve the generalization ability of machine learning models.</p>',5),Z={href:"https://en.wikipedia.org/wiki/Dimensionality_reduction",target:"_blank",rel:"noopener noreferrer"},J=e("h4",{id:"parameters-4",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#parameters-4","aria-hidden":"true"},"#"),a(" Parameters")],-1),K=e("code",null,"method",-1),X={href:"https://en.wikipedia.org/wiki/Principal_component_analysis",target:"_blank",rel:"noopener noreferrer"},$={href:"https://en.wikipedia.org/wiki/Linear_discriminant_analysis",target:"_blank",rel:"noopener noreferrer"},ee=e("li",null,[e("code",null,"n_components"),a(": Number of components to keep. Data types are numeric.")],-1),ae=n('<h4 id="previews-4" tabindex="-1"><a class="header-anchor" href="#previews-4" aria-hidden="true">#</a> Previews</h4><p><img src="'+v+'" alt=""></p><h3 id="sample-balancing" tabindex="-1"><a class="header-anchor" href="#sample-balancing" aria-hidden="true">#</a> Sample Balancing</h3><h4 id="description-5" tabindex="-1"><a class="header-anchor" href="#description-5" aria-hidden="true">#</a> Description</h4><p>Sample balance refers to the adjustment of sample sizes in each category of a dataset to achieve a more even distribution. In machine learning and data analysis, sample balance is an important preprocessing step because imbalanced datasets can lead to biased results and affect the performance and accuracy of models.</p><p>Sample balance can be achieved through various methods, such as undersampling, oversampling, and synthetic sampling. Undersampling involves randomly removing samples from the class with a larger sample size to achieve balance. Oversampling involves duplicating or synthesizing new samples for the class with a smaller sample size. Synthetic sampling involves using generative models such as SMOTEENN to generate new samples to increase sample size and balance different classes.</p><p>Sample balance can improve model performance and accuracy while reducing bias due to imbalanced samples. However, excessive sample balance may lead to overfitting, so it is necessary to adjust and balance based on specific situations.</p>',7),te={href:"https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis",target:"_blank",rel:"noopener noreferrer"},ie=e("h4",{id:"parameters-5",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#parameters-5","aria-hidden":"true"},"#"),a(" Parameters")],-1),ne=e("code",null,"balancing_method",-1),re=e("code",null,"undersample",-1),oe={href:"https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html",target:"_blank",rel:"noopener noreferrer"},se=e("code",null,"oversample",-1),de={href:"https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html",target:"_blank",rel:"noopener noreferrer"},le=e("code",null,"combined",-1),ce={href:"https://imbalanced-learn.org/stable/references/generated/imblearn.combine.SMOTEENN.html",target:"_blank",rel:"noopener noreferrer"},he=n('<h4 id="previews-5" tabindex="-1"><a class="header-anchor" href="#previews-5" aria-hidden="true">#</a> Previews</h4><p><img src="'+k+'" alt=""></p><h3 id="normalization" tabindex="-1"><a class="header-anchor" href="#normalization" aria-hidden="true">#</a> Normalization</h3><h4 id="description-6" tabindex="-1"><a class="header-anchor" href="#description-6" aria-hidden="true">#</a> Description</h4><p>Data standardization refers to the process of transforming raw data to conform to a certain standard or specification, making it easier to compare and analyze. The most common method of data standardization is to transform the data into a standard normal distribution with a mean of 0 and a variance of 1, or to scale the data within a certain range.</p><p>The main purpose of data standardization is to eliminate differences in units and scales among the data, making comparisons between different variables more fair. For example, if two variables have vastly different ranges of values, the comparison between them may be subject to significant errors. Standardizing the data can eliminate such errors and make the data more reliable and interpretable.</p><p>Data standardization is a necessary step in many data analysis and machine learning tasks, such as clustering analysis, regression analysis, and neural networks. Common methods of data standardization include z-score normalization, min-max normalization, and mean-variance normalization.</p>',7),pe={href:"https://en.wikipedia.org/wiki/Normalization_(statistics)",target:"_blank",rel:"noopener noreferrer"},me=e("h4",{id:"parameters-6",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#parameters-6","aria-hidden":"true"},"#"),a(" Parameters")],-1),ue=e("code",null,"Method",-1),ge={href:"https://en.wikipedia.org/wiki/Feature_scaling",target:"_blank",rel:"noopener noreferrer"},fe={href:"https://en.wikipedia.org/wiki/Standard_score",target:"_blank",rel:"noopener noreferrer"},_e=e("h4",{id:"previews-6",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#previews-6","aria-hidden":"true"},"#"),a(" Previews")],-1),be=e("p",null,[e("img",{src:s,alt:""})],-1);function ye(we,ve){const o=r("RouterLink"),i=r("ExternalLinkIcon");return D(),T("div",null,[j,e("p",null,[a("In the pop-up window, you will need to select a dataset (Want to upload your own dataset? Please refer to the "),t(o,{to:"/guide/my-data.html#upload-files"},{default:P(()=>[a("My Data")]),_:1}),a(" user manual to learn how to upload a dataset). Then click on the "),z,a(" button in the lower right corner to start data processing. The system will automatically create a data processing project for you.")]),I,e("blockquote",null,[e("p",null,[e("a",N,[a("Outlier - Wikipedia"),t(i)])])]),O,e("ul",null,[e("li",null,[C,a(": Outlier detection method. currently supported "),e("a",E,[a("3-sigma"),t(i)]),a(", "),e("a",M,[a("IQR"),t(i)]),a(", and "),e("a",W,[a("MAD"),t(i)]),a(" three methods.")]),F]),q,e("blockquote",null,[e("p",null,[e("a",A,[a("Missing data - Wikipedia"),t(i)])])]),L,e("blockquote",null,[e("p",null,[e("a",R,[a("Truncation (statistics) - Wikipedia"),t(i)])]),e("p",null,[e("a",B,[a("Shrinkage (statistics) - Wikipedia"),t(i)])])]),H,e("blockquote",null,[e("p",null,[e("a",U,[a("Data transformation (statistics) - Wikipedia"),t(i)])])]),V,e("ul",null,[e("li",null,[G,a(": Data conversion method. Currently, "),e("a",Y,[a("FFT"),t(i)]),a(" and IFFT (Inverse Fast Fourier Transform) are supported.")])]),Q,e("blockquote",null,[e("p",null,[e("a",Z,[a("Dimensionality reduction - Wikipedia"),t(i)])])]),J,e("ul",null,[e("li",null,[K,a(": Dimensionality reduction method. Currently supports "),e("a",X,[a("PCA"),t(i)]),a(" and "),e("a",$,[a("LDA"),t(i)]),a(" two methods.")]),ee]),ae,e("blockquote",null,[e("p",null,[e("a",te,[a("Oversampling and undersampling in data analysis - Wikipedia"),t(i)])])]),ie,e("ul",null,[e("li",null,[ne,a(": Sample balancing method. Currently supports "),re,a(" ("),e("a",oe,[a("RandomUnderSampler"),t(i)]),a("), "),se,a(" ("),e("a",de,[a("RandomOverSampler"),t(i)]),a(") and "),le,a(" ï¼ˆ"),e("a",ce,[a("SMOTEENN"),t(i)]),a(") three methods.")])]),he,e("blockquote",null,[e("p",null,[e("a",pe,[a("Normalization (statistics) - Wikipedia"),t(i)])])]),me,e("ul",null,[e("li",null,[ue,a(": Normalization method. Currently supports "),e("a",ge,[a("Min_Max"),t(i)]),a(" and "),e("a",fe,[a("Z_Score"),t(i)]),a(" two methods.")])]),_e,be])}const De=x(S,[["render",ye],["__file","data-processing.html.vue"]]);export{De as default};
